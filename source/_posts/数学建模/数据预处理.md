---
title: 数据预处理
tags: [数学建模, 机器学习, 数据处理]
categories: 数学建模
date: 2023-03-01 19:20:00
---
### 导包
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```
```python
# 支持中文
plt.rcParams['font.sans-serif'] = ['SimSun']  # 用来正常显示中文标签
plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号
plt.rcParams['font.size'] = 12 # 小四字体
```
### 数据导入
```python
import pandas as pd

df = pd.read_csv('./data/sh600000.csv',  encoding='gbk',)
df
```
### 数据处理常用函数
```python
# 结果显示每个取值在每列中的出现次数
df.value_counts() 
# 按行合并数据
merged_df = pd.concat([df1, df2, df3], axis=0, ignore_index=True)
# 删除某一列数据
df = df.drop(labels, axis = 1)
# 索引重新排序
df.index = range(len(df))
df = df.reset_index(drop=True)
# 导出数据
df.to_csv("filename.csv", index=False)
# 替换数据
df["*"] = df["*"].replace({'*': 1})
# sql中内链接
pd.merge(df1, df2, on='key')
# 将字符串数据转换为日期数据
df3_2 = df3_1
df3_2['销售日期'] = pd.to_datetime(df3_2['销售日期'])
df3_2
# 按照日期数据从远到近排序
df = df.sort_values('date', ascending=False)
# SQL group by 操作
df3_2 = df3_1.groupby(['销售日期','分类名称', '单品名称']).agg({'销量(千克)': np.sum})
```
### 处理异常值

```python
# 处理异常值
def sigma3(data):
    col = data.columns.to_list()
    for c in col:

        # 计算'income'列的平均值和标准差
        mean = data[c].mean()
        std = data[c].std()

        # 计算异常值的范围
        lower = mean - 3 * std
        upper = mean + 3 * std

        # 筛选出超出异常值范围的数据点
        outliers = data[(data[c] < lower) | (data[c] > upper)]

        # 删除异常值
        data = data.drop(outliers.index)
        return data
```
### 定类数据标签化

```py
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
data['定类的数据'] = label_encoder.fit_transform(data['定类的数据'])
```



### 数据分割

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)
```
### 标准化
```python
from sklearn.preprocessing import StandardScaler

scaler  = StandardScaler()
X_train = scaler .fit_transform(X_train)
X_test = scaler .transform(X_test)
```
### 文本特征提取
```python
# 转换成字典 同时完成独热编码
X = X.to_dict(orient="records")

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)

from sklearn.feature_extraction import DictVectorizer
dv = DictVectorizer()
X_train = dv.fit_transform(X_train)
X_test = dv.transform(X_test)
# feature_names=dv.get_feature_names()
```
### 模型建立的基本代码
```python
from sklearn.* import *

estimator = *()
estimator.fit(X_train, y_train)
y_predict = estimator.predict(X_test)
print("Train set score: ", estimator.score(X_train, y_train))
print("Test set score: ", estimator.score(X_test, y_test))
```
### 主成分分析法（PCA）
### 注意

1. 进行主成分分析法之前，需要进行数据标准化
```python
from sklearn.preprocessing import StandardScaler

scaler  = StandardScaler()
data_test = scaler .fit_transform(data_test)
```
#### 代码
```python
from sklearn.decomposition import PCA
pca = PCA(n_components=0.95)
data_new = pca.fit_transform(data)
```
#### 属性

1. **components_：**
```python
# 特征系数
pca.components_
```

2. 各个维度的贡献率：
```python
# 各个维度的贡献率
explained_var = pca.explained_variance_ratio_
explained_var
```

3. 累计贡献率
```python
# 累计贡献率
import numpy as np
np.cumsum(pca.explained_variance_ratio_)

# 绘图
plt.plot(range(n), np.cumsum(explained_var))
plt.ylim([0, 1.1])
```
#### 参数

1. **n_components：**
   1. 小数：表示保留百分之多少的信息
   2. 整数：减少到多少特征
### 相关性分析
#### 绘制散点图矩阵
通过散点的分布来确定相关性
```python
import matplotlib.pyplot as plt

#绘制散点图矩阵
pd.plotting.scatter_matrix(data)
plt.show()
```
#### 相关系数矩阵
##### pandas.corr 函数（无显著性检验）
```python
import pandas as pd

pearson_corr = data.corr(method="pearson")
pearson_corr
```
##### scipy.stats.pearsonr 函数 （有显著性检验）
```python
from scipy.stats import pearsonr

r = pearsonr(df['X'],df['Z'])
print("pearson系数：",r[0])
print("   P-Value：",r[1])
```
#### 相关矩阵热力图
```python
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure()
sns.heatmap(data.corr(method='pearson'),annot=True,square=True,vmax=1,cmap="Oranges")
plt.title('皮尔逊热力图')
plt.show()
```
### 
