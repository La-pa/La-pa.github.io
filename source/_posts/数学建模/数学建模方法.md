---
title: 数学建模方法
tags: [数学建模, 机器学习, 数据处理]
categories: 数学建模
date: 2023-03-01 19:20:00
---
## 灰色相关分析
### 代码
链接：[https://blog.csdn.net/weixin_51545953/article/details/111029419?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167877326916800182185578%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=167877326916800182185578&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-111029419-null-null.142^v73^insert_down2,201^v4^add_ask,239^v2^insert_chatgpt&utm_term=%E7%81%B0%E8%89%B2%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90](https://blog.csdn.net/weixin_51545953/article/details/111029419?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167877326916800182185578%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=167877326916800182185578&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-111029419-null-null.142^v73^insert_down2,201^v4^add_ask,239^v2^insert_chatgpt&utm_term=%E7%81%B0%E8%89%B2%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90)
```matlab
%% 应用一：分析产业对GDP的影响程度
clear;clc;
load data.mat;
r = size(data,1);
c = size(data,2);
%第一步，对变量进行预处理，消除量纲的影响（大家在使用时需要注意自己的数据量纲是否相同）
%avg = repmat(mean(data),r,1);
%data = data./avg;
%定义母序列和子序列
Y = data(:,1); %母序列
X = data(:,2:c); %子序列
Y2 = repmat(Y,1,c-1); %把母序列向右复制到c-1列
absXi_Y = abs(X-Y2)
a = min(min(absXi_Y)) %全局最小值
b = max(max(absXi_Y)) %全局最大值
ro = 0.5; %分辨系数取0.5
gamma = (a+ro*b)./(absXi_Y+ro*b) %计算子序列中各个指标与母序列的关联系数
disp("子序列中各个指标的灰色关联度分别为：");
ans = mean(gamma)

```
```matlab
%应用二：灰色关联分析评价河流情况
clear;clc;
load X.mat;
%获取行数列数
r = size(X,1);
c = size(X,2);
%首先，把我们的原始指标矩阵正向化
%第二列中间型--->极大型
middle = input("请输入最佳的中间值：");
M = max(abs(X(:,2)-middle));
for i=1:r
X(i,2) = 1-abs(X(i,2)-middle)/M;
end
%第三列极小型--->极大型
max_value = max(X(:,3)); 
X(:,3) = abs(X(:,3)-max_value);
%第四列区间型--->极大型
a = input("请输入区间的下界：");
b = input("请输入区间的下界：");
M = max(a-min(X(:,4)),max(X(:,4))-b);
for i=1:r
if (X(i,4)<a)
X(i,4) = 1-(a-X(i,4))/M;
elseif (X(i,4)<=b&&X(i,4)>=a)
X(i,4) = 1;
else
X(i,4) = 1-(X(i,4)-b)/M;
end
end
disp("正向化后的矩阵为：");
disp(X);
%把正向化后的矩阵进行预处理，消除量纲的影响
avg = repmat(mean(X),r,1);
new_X = X./avg;
%将预处理后的矩阵每一行的最大值取出，当成母序列(虚构的)
Y = max(new_X,[],2);
%计算各个指标和母序列的灰色关联度
%先把new_X矩阵所有元素都减去母序列中同行的元素，并取绝对值
Y2 = repmat(Y,1,c);
new_X = abs(new_X-Y2);
a = min(min(new_X)); %全矩阵最小值
b = max(max(new_X)); %全矩阵最大值
ro = 0.5;
new_X = (a+ro*b)./(new_X+ro*b);
disp("各个指标对于母序列的灰色关联度为：");
gamma = mean(new_X)
%计算各个指标的权重
disp("各个指标的权重为：");
weight = gamma./(sum(gamma,2))

%-------------------------------------------------------------------------------------------------------
%继续TOPSIS的步骤：对正向化后的矩阵X进行标准化（原矩阵除以每一列元素平方之和的开方）
temp1 = X.*X;               %先让每每一个元素平方
temp2 = sum(temp1);         %再对每一列求和
temp3 = temp2.^0.5;         %再把结果开方
temp4 = repmat(temp3,r,1);  %把开方后的结果按行复制r行
disp("******标准化后的矩阵为：");
Z = X./temp4               %原矩阵除以每一列元素平方之和的开方
Z_max = max(Z)           %获得Z每一列中最大的元素
Z_min = min(Z)           %获得Z每一列中最小的元素
D_max = sum(weight.*(Z-repmat(Z_max,r,1)).^2,2).^0.5
D_min = sum(weight.*(Z-repmat(Z_min,r,1)).^2,2).^0.5
disp("该矩阵得分为：")
S = D_min./(D_max+D_min)
disp("矩阵归一化后得分为：");
S = S./(repmat(sum(S),r,1))

```

## TOPSIS（优劣解距离法）
### 链接
[https://blog.csdn.net/zedkyx/article/details/125258125?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167877524316782428655159%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=167877524316782428655159&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-125258125-null-null.142^v73^insert_down2,201^v4^add_ask,239^v2^insert_chatgpt&utm_term=%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95TOPSIS&spm=1018.2226.3001.4187](https://blog.csdn.net/zedkyx/article/details/125258125?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167877524316782428655159%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=167877524316782428655159&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-125258125-null-null.142^v73^insert_down2,201^v4^add_ask,239^v2^insert_chatgpt&utm_term=%E4%BC%98%E5%8A%A3%E8%A7%A3%E8%B7%9D%E7%A6%BB%E6%B3%95TOPSIS&spm=1018.2226.3001.4187)
## 典型相关分析（CCA）
```python
## 通过sklearn工具包内置的CCA实现
import numpy as np
from sklearn.cross_decomposition import CCA
from icecream import ic   # ic用于显示，类似于print

A = [[3, 4, 5, 6, 7] for i in range(2000)] 
B = [[8, 9, 10, 11, 12] for i in range(2000)] 
# 注意在A、B中的数为输入变量及输出变量参数

# 建模
cca = CCA(n_components=1)  # 若想计算第二主成分对应的相关系数，则令cca = CCA(n_components=2)
# 训练数据
cca.fit(X, Y)
# 降维操作
X_train_r, Y_train_r = cca.transform(X, Y)
#输出相关系数


ic(np.corrcoef(X_train_r[:, 0], Y_train_r[:, 0])[0, 1])  
#如果想计算第二主成分对应的相关系数 print(np.corrcoef(X_train_r[:, 1], Y_train_r[:, 1])[0, 1])

```

## 时间序列
### ARIMA 模型
#### 链接
[ARIMA（p,d,q）模型原理及其实现 --------python_arima python_English Chan的博客-CSDN博客](https://blog.csdn.net/weixin_49583390/article/details/121914303?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522167879476916800197016726%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=167879476916800197016726&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-121914303-null-null.142^v73^insert_down2,201^v4^add_ask,239^v2^insert_chatgpt&utm_term=ARIMA)
#### 代码
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats 

##################################################################

df = pd.read_excel("ProblemC.xlsx")
df

data = df.set_index(["Date"])
data.head()

data = data[["Number in hard mode"]]
data

data.plot()

##################################################################
#平稳数据折线图
font = {"size":15,
    "family":"fangsong"}
plt.rc("font",**font)
plt.rcParams['axes.unicode_minus']=False
plt.figure(figsize=(12,4))
plt.plot(data.values,label="源数据")
plt.plot([0,120],[0,0],"--",c = "grey")
plt.xlim(0,120)
plt.legend()
plt.show()

plt.figure(figsize=(12,4))
plt.plot(data.diff(1).values,c="darkgreen",label="一阶差分")
plt.plot([0,120],[0,0],"--",c = "grey")
plt.xlim(0,120)
plt.legend()
plt.show()

##################################################################

# 确定p
from statsmodels.graphics.tsaplots import plot_pacf


fig = plt.figure(figsize=(12,8))
ax1 = fig.add_subplot(211)
plot_pacf(data.diff(1).values[1:], lags=40,ax = ax1)

# 确定q

from statsmodels.graphics.tsaplots import plot_acf
ax2 = fig.add_subplot(212)
# plt.figure(figsize=(12,4))
plot_acf(data.diff(1).values[1:], lags=40,ax = ax2)
plt.show()

#######################################################################

# 根据判断出来的p和q还有d建立模型(p,d,q)

# 拆分数据集为训练集和测试集
from sklearn.model_selection import train_test_split
train, test = train_test_split(data, test_size=0.2, shuffle=False)
train

####################################################################

# 对测试集进行预测并计算误差
from statsmodels.tsa.arima.model import ARIMA
history = [x for x in train.values]
predictions = list()
for t in range(len(test)):
    model = ARIMA(history, order=(3, 1, 1))
    model_fit = model.fit()
    output = model_fit.forecast()
    yhat = output[0]
    predictions.append(yhat)
    obs = test.values[t]
    history.append(obs)
    
#################################################################

from sklearn.metrics import mean_squared_error
RMSE_test = np.sqrt(mean_squared_error(test, predictions))
print("Test RMSE: {}".format(RMSE_test))

##################################################################

plt.plot(test.index, test.values, label='Actual')
plt.plot(test.index, predictions, label='Predicted')
plt.title('Daily Wordle Results')
plt.xlabel('Date')
plt.ylabel('Number of reported results')
plt.legend()
plt.show()

#####################################################################
# 对未来数据进行预测
forecast = model_fit.forecast(steps=30)
forecast
```
